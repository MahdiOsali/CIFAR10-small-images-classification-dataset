# ğŸ¯ Exploring CNN Architectures on CIFAR-10

In this project, I experimented with **Convolutional Neural Networks (CNNs)** on the **CIFAR-10 dataset**.  
The focus was to analyze how different architectural choices and training strategies impact model performance.

---

## ğŸ—ï¸ Network Architecture
The main CNN model consisted of:
- **Conv2D (32 filters, kernel size 5Ã—5, ReLU)** + BatchNorm  
- **Conv2D (32 filters, kernel size 3Ã—3, ReLU)** + BatchNorm  
- **MaxPooling2D (2Ã—2)**  
- **Conv2D (68 filters, kernel size 3Ã—3, ReLU)** + BatchNorm  
- **Conv2D (68 filters, kernel size 3Ã—3, ReLU)** + BatchNorm  
- **MaxPooling2D (2Ã—2)**  
- **Flatten â†’ Dense (128 units, ReLU) â†’ Dropout(0.3) â†’ Dense (10 units, Softmax)**  

---

## â±ï¸ Training Experiments
I tested the model under different setups (50 epochs each):  
- âœ… **With Pooling Layer** â†’ Faster training and higher accuracy.  
- âŒ **Without Pooling Layer** â†’ Slower training, lower accuracy.  
- âš¡ **Stride Comparison (2 vs 4)** in pooling:  
  - Stride = 2 â†’ better accuracy and stable training.  
  - Stride = 4 â†’ information loss, lower accuracy.  
- ğŸ“‰ **Schedulers (Exponential vs OneCycle)**:  
  - Exponential Decay â†’ slower but stable convergence.  
  - OneCycle Policy â†’ faster convergence, improved final accuracy.  

---

## âœ… Key Takeaways
- Pooling layers play a **critical role** in reducing training time and improving accuracy.  
- Proper stride selection (e.g., 2 instead of 4) significantly improves performance.  
- The **OneCycle Scheduler** outperformed Exponential Decay in both speed and accuracy.  
- CNNs proved to be **efficient and effective** on CIFAR-10 classification tasks.  

---

ğŸ”— Excited to keep exploring **Deep Learning** and **Computer Vision**.  
Iâ€™d love to hear your thoughts and experiences with CNNs on image datasets!
